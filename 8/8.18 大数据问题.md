## 8.18 大数据问题

### 数据流采样
有一个无限的整数数据流，如何从中随机地抽取k个整数出来？

这是一个经典的数据流采样问题，我们一步一步来分析。

我们先考虑最简单的情况k=1，即只需要随机抽取一个样本出来。抽样方法如下：
* 当第一个整数到达时，保存该整数
* 当第2个整数到达时，以1/2的概率使用该整数替换第1个整数，以1/2的概率丢弃改整数
* 当第i个整数到达时，以1/i​​的概率使用第i个整数替换被选中的整数，以1-1/i的概率丢弃第i个整数

假设数据流目前已经流出共n个整数，这个方法能保证每个元素被选中的概率是1/n​​吗？用数学归纳法，证明如下：
* 当n=1时，由于是第1个数，被选中的概率是100%，命题成立
* 假设当n=m(m>=1)时，命题成立，即前m个数，每一个被选中的概率是1/m
* 当n=m+1时，第m+1个数被选中的概率是1/​m+1​, 前m个数被选中的概率是(1/m)*(1-1/m+1)，命题依然成立
由1，2，3知n>=1时命题成立，证毕。

同样当k>1时，需要随机采样多个样本时，方法跟上面很类似，
* 前k个整数到达时，全部保留，即被选中的概率是100%，
* 第i个整数到达时，以k/i的概率替换k个数中的某一个，以1-k/i​的概率丢弃该数，保留k个数不变

### 相同url
给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url？

#### 分治
可以估计每个文件的大小为50G×64=320G，远远大于内存限制的4G。所以不可能将其完全加载到内存中处理。考虑采取分治的方法：
* 遍历文件a，对每个url求取`hash(url)%1000`，然后根据所取得的值将url分别存储到1000个小文件a0...a999，这样每个小文件的大约为300M；同样方法处理文件b。
* 这样处理后，所有可能相同的url都在对应的小文件中，不对应的小文件不可能有相同的url。然后我们只要求出1000对小文件中相同的url即可。
* 求每对小文件中相同的url时，可以把其中一个小文件的url都存储到一个set中，然后遍历另一个小文件的每个url即可。

#### BloomFilter  
如果允许有一定的错误率，可以使用BloomFilter，4G内存大概可以表示340亿bit。首先使用文件a中的url构造一个BloomFilter，然后使用这个Filter处理文件b即可。

### 频率排序
有10个文件，每个文件1G，每个文件的每一行存放的都是用户的query，每个文件的query都可能重复。要求你按照query的频度排序。

本题也可以使用分治的方案解决，关键是将同一个query分到一个子问题中：
* 顺序读取10个文件，按照`hash(query)%10`的结果将query写入到另外10个文件a0...a9中，这样相同的query就会保存到同一个文件中。新生成的文件每个的大小大约也1G（假设hash结果是均匀的）。
* 然后使用map来统计每个query出现的次数，并根据出现次数进行排序，然后将排好序的query和对应的count保存到一个新文件b0...b9中。
* 最后对b0...b9进行归并排序（由于每个文件已经是有序的了，这一步只需要归并即可）。

### Top k
1亿个数中找出最大的100个数。

典型的Top k问题，可以使用堆来解决：
* 首先读入前100个数来创建大小为100的最小堆，建堆的时间复杂度为O（mlogm）。
* 然后遍历后续的数字，并和堆顶（最小）数字进行比较。如果比最小的数小，则继续读取后续数字；如果比堆顶数字大，则替换堆顶元素并重新调整堆为最小堆。整个过程直至1亿个数全部遍历完为止。
* 然后按照中序遍历的方式输出当前堆中的所有100个数字。

该算法的时间复杂度为O（nmlogm），空间复杂度是100（常数）。

### Top k频率
1000万个字符串，求出频数最高的100个词。

仍然是采用分治+map+heap方案：
* 顺序读文件中，对于每个词x求取`hash(x)%1000`，然后按照该值存到1000个小文件a0...a999中（根据内存情况还可以继续向下分解）。
* 对每个小文件，统计每个文件中出现的词以及相应的频率（可以采用map或者trie），然后取出频率最大的100个单词（可以使用含100个结点的最小堆），并把100词及相应的频率存入文件，这样又得到了1000个文件。
* 最后将这1000个文件进行归并求出出现频率最大的100个单词。

### 去重
1000万字符串，其中有些是重复的，需要把重复的全部去掉，保留没有重复的字符串。

仍然是分治+map的方案：
* 顺序读文件中，对于每个词x求取`hash(x)%1000`，然后按照该值存到1000个小文件a0...a999中。
* 对每个小文件，统计每个文件中出现的词以及相应的频率（可以采用map或者trie），然后再次遍历该文件找到不重复单词并输出到文件中。
* 最后将这1000个文件进行归并求出所有不重复的单词。

### 不重复整数
在1亿个整数中找出不重复的整数，内存不足以容纳这1亿个整数。

由于是整数去重的问题，可以采用BitMap的方案：
* 采用2-bitmap（每个数分配2bit，00表示不存在，01表示出现一次，10表示多次，11无意义）进行，共需内存2^32*2bit=1GB内存（如果我们知道最大值，内存还可以再节省一些）。
* 然后扫描这1亿个整数，查看bitmap中相对应位，如果是00变01，01变10，10保持不变。
* 最后遍历该bitmap，把对应位是01的整数输出即可。

### 中位数
找到1亿个整数的中位数。可以采用计数的方法：
* 一个整数假设是32位无符号数，第一次扫描把0~2^32-1分成2^16个区间，统计每个区间的整数数量，找出中位数具体所在区间以及在该区间内的offset。
* 然后取出该区间第offset个数即可。

### 总结
下面我们总结一下大数据的常用工具：
* BitMap：数字排序，判重（2bit）。比较适合能够完美Hash的情况，比如正整数
* BloomFilter：判重问题，无法完美Hash
* 分治归并：适合大多数大数据问题，问题的关键是划分子问题
* 堆：适合Top k问题，常量空间，一遍统计即可完成
* Map：频率统计
* Trie：频率统计，前缀匹配等

